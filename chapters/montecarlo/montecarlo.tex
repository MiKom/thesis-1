\chapter{Monte Carlo integration}
Monte Carlo integration is a method of approximating the value of an integral which is difficult or impossible to solve analytically, thus it is usually the solution of choice for approximating the \emph{light transport equation} \footnote{While there are other valid approaches to solving the LTE such as finite element methods (e.g. radiosity), those are not in the scope of this work.}. Due to the high dimensionality of the LTE integral traditional numerical integration methods like quadrature rules are inefficient and converge very slowly to the approximate solution.

\section{The Monte Carlo estimator}
Let $f: I \rightarrow \mathbb{R}$ be a real-valued function defined over the interval $I$ and $a,b \in I$. Monte Carlo integration is a mean to estimate the integral of the form:
\begin{equation}
  F = \int_{a}^{b} f(x)dx.
\end{equation}

The basis for deriving the Monte Carlo estimator is the \emph{mean value theorem} for integration:
\begin{thm}
If $f(x)$ is continuous over $[a, b]$, then there exists a number $c \in (a, b)$ such that:
\begin{equation}
\label{eq:meanvalue}
  \frac{1}{b-a} \int_{a}^{b} f(x)dx = f(c).
\end{equation}
\end{thm}
Rearranging the terms of equation \ref{eq:meanvalue} yields:
\begin{equation}
  \int_{a}^{b} f(x)dx = (b-a) \cdot f(c) = F,
\end{equation}
which has simple interpretation that the area under the curve is the width of its base $(b-a)$ times the average ``height'' $f(c)$. To estimate the value of $f(c)$ one could evalute the function $f(x)$ at random points in the interval $[a,b]$ and compute the average of the sum of samples.

\begin{df}[Monte Carlo estimator]
  Let $X_{i} \in [a,b]$ be a random variable drawn from some distribution with probability density function $p(X_{i})$ and let $f: I \rightarrow \mathbb{R}$ be a function defined over the interval $I$, and $a,b \in I$. The Monte Carlo estimator has the form \parencite{veach97}:
\begin{equation}
\label{eq:mcestimator}
  F_{N} = \frac{1}{N} \sum_{i=1}^{N} \frac{f(X_{i})}{p(X_{i})}.
\end{equation}
\end{df}
With that definition the expected value of the estimator $F_{N}$ is the value of the integral $F$:
\begin{eqnarray}
  E[F_{N}] &=& E\left[ \frac{1}{N} \sum_{i=1}^{N} \frac{f(X_{i})}{p(X_{i})} \right]
  = \frac{1}{N} \sum_{i=1}^{N} E \left[ \frac{f(X_{i})}{p(X_{i})} \right] = \nonumber \\
  &=& \frac{1}{N} \sum_{i=1}^{N} \int_{a}^{b} \frac{f(x)}{p(x)} p(x) dx
  = \frac{1}{N} \sum_{i=1}^{N} \int_{a}^{b} f(x) dx = \nonumber \\
  &=& \int_{a}^{b} f(x)dx = F.
\end{eqnarray}
For functions with higher number of dimensions random samples $X_{i}$ are drawn from \emph{joint probability density function}.

Average case rate of convergence for Monte Carlo estimator is $O(\sqrt{N})$, where $N$ is the number of samples taken. Formal variance and error analysis of Monte Carlo integration is out of scope of this work. Treatment of this topic can be found in \cite{robert2004}.

\section{Sampling from arbitrary distributions}
Vast majority of pseudo-random number generation algorithms output numbers drawn from \emph{standard uniform distribution}. Random variable with such distribution is commonly written as $\xi \in [0,1)$ and it takes on all values in its domain with equal probability.

Ability to draw random samples from chosen probability distribution is crucial in order to evaluate the Monte Carlo estimator in equation \ref{eq:mcestimator}. This section will present basics of drawing samples from arbitrary distributions given only samples drawn from the standard uniform distribution.

\subsection{The inversion method}
The inversion method or the \emph{inverse transform sampling} \parencite{devroye86} is a method of generating samples according to some distribution by transforming the canonical uniform random variable $\xi$.
\begin{thm}[The inversion principle]
  Let $P(x)$ be a continuous distribution function on $\mathbb{R}$ with inverse $P^{-1}$ defined by
  \begin{equation}
    P^{-1}(u) = \inf \left\{ x:P(x)=u, 0 < u < 1 \right\}.
  \end{equation}
If $\xi \in [0,1]$ is a uniform random variable, then $P^{-1}(\xi)$ has distribution function $P$.
\end{thm}

Let $\xi_{i}$ be a uniformly distributed random number obtained via some pseudo-random number generator (RNG). Given the above theorem it is possible to draw a sample $X_{i}$ from an arbitrary probability density function $p(x)$ by transforming $\xi_{i}$ in the following way:
\begin{enumerate}
\item Compute the cumulative distribution function $P(x) = \int_{-\infty}^{x} p(x') \,dx'$.
\item Find the inverse $P^{-1}(x)$.
\item Compute $X_{i} = P^{-1}(\xi_{i})$.
\end{enumerate}

\subsection{Transforming between distributions}
When sampling from multidimensional distributions it is often useful to transform a random variable with some function $f$. For example one might wish to express solid angles in terms of points in spherical coordinates. When dealing with such transformations it is curcial to know the relationship between corresponding density functions.

Consider a pair of n-dimensional random variables $X$, $Y$ with corresponding probability density functions $p_{x}(x)$ and $p_{y}(y)$. Let $Y = T(X)$ where $T(x)=\langle T_{1}(x), \dots, T_{n}(x) \rangle$ is a bijection. The densities are then related by \parencite{phar2010}:
\begin{equation}
  p_{y}(y)=p_{y}(T(x)) = \frac{p_{x}(x)}{|J_{T}(x)|},
\end{equation}
where $|J_{T}(x)|$ is the absolute value of the determinant of $T$'s Jacobian matrix.

\section{2D sampling of random variables}
A number of two-dimensional distributions are commonly used when approximating solution to the LTE using Monte Carlo integration. This section introduces methods of uniformly sampling those distributions.

\subsection{Unit hemisphere}
Consider choosing a direction vector on a hemisphere uniformly with respect to solid angle. Uniform distribution implies that the density function is constant and it also, by definition, integrates to one.
\begin{eqnarray}
  \int_{\Omega} p(\omega) \,d\omega &=& 1 \nonumber \\
  c \int_{\Omega} \,d\omega &=& 1 \nonumber \\
  p(\omega) = c &=& \frac{1}{2\pi}
\end{eqnarray}
The next step is to express the density function with respect to spherical coordinates using equation \ref{eq:dw2spherical}:
\begin{eqnarray}
  p(\theta, \phi) \, d\theta d\phi &=& p(\omega) \,d\omega \nonumber \\
  p(\theta, \phi) &=& \sin\theta \,p(\omega),
\end{eqnarray}
subsituting for $p(\omega)$ yields
\begin{equation}
  p(\theta, \phi) = \sin\theta \,\frac{1}{2\pi}.
\end{equation}
It is easly seen that the two-dimensional density function $p(\theta, \phi)$ is seperable and thus can be expressed as a product of two one-dimensional densities: $p(\theta) = \sin\theta$ and $p(\phi) = \frac{1}{2\pi}$ which can be sampled independently using the inversion method.

Computing the respective cumulative distribution functions:
\begin{eqnarray}
  P(\theta) &=& \int_{-\infty}^{\theta} \sin\theta' \,d\theta' = 1 - \cos\theta \\
  P(\phi) &=& \int_{-\infty}^{\phi} \frac{1}{2\pi} \,d\phi' = \frac{\phi}{2\pi},
\end{eqnarray}
and their inversions yields:
\begin{eqnarray}
  \theta &=& P^{-1}(\theta) = \cos^{-1}(1 - \xi_{1}) \label{eq:2dsample_theta} \\
  \phi &=& P^{-1}(\phi) = 2\pi \xi_{2},
\end{eqnarray}
where $\xi_{1}$ and $\xi_{2}$ are random numbers drawn from standard uniform distribution. Note that if $\xi_{i}$ is uniformly distributed random number over $[0,1]$ then $1-\xi_{i}$ is also uniformly distributed. This can be used to further simplify equation \ref{eq:2dsample_theta}:
\begin{equation}
  \theta = \cos^{-1}(1-\xi_{1}) = \cos^{-1}(\xi_{1}).
\end{equation}
Converting $\theta$ and $\phi$ into Cartesian coordinates yields final formulae for uniformly sampling a hemisphere:
\begin{eqnarray}
  x &=& \sin\theta \cos\phi = \cos(2\pi \xi_{2}) \sqrt{1 - \xi_{1}^{2}} \nonumber \\
  y &=& \sin\theta \sin\phi = \sin(2\pi \xi_{2}) \sqrt{1 - \xi_{1}^{2}} \\
  z &=& \cos\theta = \xi_{1}. \nonumber
\end{eqnarray}

\subsection{Unit disk}

\subsection{Cosine weighted unit hemisphere}

