\chapter{Monte Carlo integration}
Monte Carlo integration is a method of approximating the value of an integral which is difficult or impossible to solve analytically, thus it is usually the solution of choice for approximating the \emph{light transport equation} \footnote{While there are other valid approaches to solving the LTE such as finite element methods (e.g. radiosity), those are not in the scope of this work.}. Due to the high dimensionality of the LTE integral traditional numerical integration methods like quadrature rules are inefficient and converge very slowly to the approximate solution.

\section{The Monte Carlo estimator}
Let $f: I \rightarrow \mathbb{R}$ be a real-valued function defined over the interval $I$ and $a,b \in I$. Monte Carlo integration is a mean to estimate the integral of the form:
\begin{equation}
  F = \int_{a}^{b} f(x)dx.
\end{equation}

The basis for deriving the Monte Carlo estimator is the \emph{mean value theorem} for integration:
\begin{thm}
If $f(x)$ is continuous over $[a, b]$, then there exists a number $c \in (a, b)$ such that:
\begin{equation}
\label{eq:meanvalue}
  \frac{1}{b-a} \int_{a}^{b} f(x)dx = f(c).
\end{equation}
\end{thm}
Rearranging the terms of equation \ref{eq:meanvalue} yields:
\begin{equation}
  \int_{a}^{b} f(x)dx = (b-a) \cdot f(c) = F,
\end{equation}
which has simple interpretation that the area under the curve is the width of its base $(b-a)$ times the average ``height'' $f(c)$. To estimate the value of $f(c)$ one could evalute the function $f(x)$ at random points in the interval $[a,b]$ and compute the average of the sum of samples.

\begin{df}[Monte Carlo estimator]
  Let $X_{i} \in [a,b]$ be a random variable drawn from some distribution with probability density function $p(X_{i})$ and let $f: I \rightarrow \mathbb{R}$ be a function defined over the interval $I$, and $a,b \in I$. The Monte Carlo estimator has the form \parencite{veach97}:
\begin{equation}
\label{eq:mcestimator}
  F_{N} = \frac{1}{N} \sum_{i=1}^{N} \frac{f(X_{i})}{p(X_{i})}.
\end{equation}
\end{df}
With that definition the expected value of the estimator $F_{N}$ is the value of the integral $F$:
\begin{eqnarray}
  E[F_{N}] &=& E\left[ \frac{1}{N} \sum_{i=1}^{N} \frac{f(X_{i})}{p(X_{i})} \right]
  = \frac{1}{N} \sum_{i=1}^{N} E \left[ \frac{f(X_{i})}{p(X_{i})} \right] = \nonumber \\
  &=& \frac{1}{N} \sum_{i=1}^{N} \int_{a}^{b} \frac{f(x)}{p(x)} p(x) dx
  = \frac{1}{N} \sum_{i=1}^{N} \int_{a}^{b} f(x) dx = \nonumber \\
  &=& \int_{a}^{b} f(x)dx = F.
\end{eqnarray}
For functions with higher number of dimensions random samples $X_{i}$ are drawn from \emph{joint probability density function}.

Average case rate of convergence for Monte Carlo estimator is $O(\sqrt{N})$, where $N$ is the number of samples taken. Formal variance and error analysis of Monte Carlo integration is out of scope of this work. Treatment of this topic can be found in \cite{robert2004}.

\section{Sampling from arbitrary distributions}
Vast majority of pseudo-random number generation algorithms output numbers drawn from \emph{standard uniform distribution}. Random variable with such distribution is commonly written as $\xi \in [0,1)$ and it takes on all values in its domain with equal probability.

Ability to draw random samples from chosen probability distribution is crucial in order to evaluate the Monte Carlo estimator in equation \ref{eq:mcestimator}. This section will present basics of drawing samples from arbitrary distributions given only samples drawn from the standard uniform distribution.

\subsection{The inversion method}
The inversion method or the \emph{inverse transform sampling} \parencite{devroye86} is a method of generating samples according to some distribution by transforming the canonical uniform random variable $\xi$.
\begin{thm}[The inversion principle]
  Let $P(x)$ be a continuous distribution function on $\mathbb{R}$. with inverse $P^{-1}$ defined by
  \begin{equation}
    P^{-1}(u) = \inf \left\{ x:P(x)=u, 0 < u < 1 \right\}.
  \end{equation}
If $\xi \in [0,1]$ is a uniform random variable, then $P^{-1}(\xi)$ has distribution function $P$.
\end{thm}

Let $\xi$ be a uniformly distributed random number obtained via some pseudo-random number generator (RNG). Given the above theorem it is possible to draw a sample $X$ from an arbitrary probability density function $p(x)$ by transforming $\xi$ in the following way:
\begin{enumerate}
\item Compute the cumulative distribution function $P(x) = \int_{-\infty}^{x} p(x') \,dx'$.
\item Find the inverse $P^{-1}(x)$.
\item Compute $X = P^{-1}(\xi)$.
\end{enumerate}

\subsection{Transforming between distributions}

\section{2D sampling of random variables}

\subsection{Rectangle}

\subsection{Unit sphere}

\subsection{Unit hemisphere}

\subsection{Cosine weighted unit hemisphere}

